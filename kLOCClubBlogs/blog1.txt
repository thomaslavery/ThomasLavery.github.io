Testcase generation

In the Microsoft article introducing the debug-gym, the authors demonstrated how granting an LLM access to just a few debugging tools significantly improved performance on SWE-bench. My intuition then led me to wonder: what if I could train LLMs to debug their code without needing to access these tools? I hoped that by directly training a model on debugging, I could increase accuracy and decrease inference times since the information could be represented internally rather than retrieved through tool calls. To get LLMs to debug their code, I thought I’d make it follow test-driven development, a skill I had learned about in class. To do this, the LLM would have to create test cases. Not just trivial cases, but cases that induced faults like overflows and race conditions. 

I didn’t believe running STF on a dataset of test cases would lead to an improvement in performance because it would be unscalable and limited to the types of test cases in the dataset. Having just read a paper on AlphaGoZero, I also wanted to remove some heuristics and leverage reinforcement learning. So I thought, I’ll give the LLM a solution with a bug in it and have it generate a test case. If the test case fails, then it is rewarded. The problem with this method is that since the model is doing everything it can to maximize the reward policy, it may cheat and create test cases that are guaranteed to fail, e.g., testing an invalid input, or not testing anything and purely inducing a crash. To combat this, I decided to only reward the model if the test case passed with a correct solution. This leads me to the goal of this project: to create a dataset consisting of problem descriptions, solutions, and erroneous solutions. 

I first scoured the web to see if there was a public dataset I could use, but the only one I came across was a LeetCode problem dataset. This, in theory, works, but it would just be training an LLM on algorithms, which it already excels at, so I decided to create my own synthetic dataset. I couldn’t simply ask an LLM to make an entry 1000 times without changing the prompt. This would create duplicate problems each time. Eventually, I came up with this solution: create one example by hand and use single-shot prompting to ask the LLM to generate an entry based on that. Then, use what it generated as the example for the next inference and repeat the process. My logic was that each prior example would shift the distribution enough to produce a unique example. Then I’d crank the temperature to increase the variance to help it along. This failed interestingly; it initially generated a couple of unique examples, but after a few iterations, it converged on two responses. Summing the even numbers in a list and determining if a string is a palindrome. I would really like to see what is happening under the hood and understand why the responses converge this way, but unfortunately, ChatGPT doesn’t let you look inside. This might be revealed in the attention blocks, but my theory is that giving an example doesn’t shift the distribution like I think it does. It changes the form of the response rather than purely the content. Meaning that if the form of the example stays the same, the output stays the same regardless of content. It alternates between outputs because it can’t copy example problems, but it is trying to provide what it thinks is the best response. So, if the best response is in the example, it gives what it thinks is the second-best response and vice versa. 

After this, I decided to dive into the literature to see if I could find a solution to my problem. First, I found a couple of articles, but they were about using LLMs to label already collected data, which didn’t work for me since I was starting from scratch. Eventually, I found this paper: https://arxiv.org/abs/2406.20094. To generate their datasets, the authors would use personas in the prompt to change what the LLM outputs.* This is highly scalable since there are a billion unique personas, each one hypothetically inducing a unique response. It is also a diversity of responses. Each persona touches a domain of knowledge and can also be thought of as a tiny fragment of society; the combination of all the personas would thus interact with a vast amount of knowledge, making the generated problem space a sizable subset of real-world issues. Unfortunately, I had no budget for my project, but I needed quite a few examples for training, so I only generated a few entries to check the results. This project will also be shelved until I can acquire the funds to move forward with training as well as generate even more complex problems, but it should be straightforward once I do. On a happy note, with this strategy, all my dataset entries came out highly unique and had depth and diversity to them. 

TLDR;
I learned to use personas to generate a synthetic dataset with breadth and depth.